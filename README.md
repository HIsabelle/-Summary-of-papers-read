# Summary-of-papers-read
학부연구생 진행 중 읽었던 논문 및 발표 정리 

## 1.Survey
|읽은 순서|Title|Year|Type|정리 링크(나만 보기용)|
|:---:|------|:---:|:---:|:------:|
|20|[A Survey on Vision Transformer](https://arxiv.org/abs/2012.12556)|2020|Pruning|
|-|[A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis and Recommendations](https://arxiv.org/abs/2308.06767)|2023|Pruning|(https://1m0n-earth.tistory.com/11)|
|-|[Structured Pruning for Deep Convolutional Neural Networks: A survey](https://arxiv.org/abs/2303.00566)|2023|Pruning|


## Pruning

|읽은 순서|Title|Venue|Year|Pruning Type|Note|정리 링크(나만 보기용)|
|:---:|------|:---:|:---:|:------:|----|:------:|
|1|[Pruning filters for efficient convnets](https://arxiv.org/abs/1608.08710)|ICLR|2017|Filter||(https://1m0n-earth.tistory.com/12)|
|5|[AMC: AutoML for Model Compression and Acceleration on Mobile Devices](https://arxiv.org/abs/1802.03494)|ECCV|2018|Filter|Search base Pruning,RL|(https://1m0n-earth.tistory.com/14)|
|6|[MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning](https://arxiv.org/abs/1903.10258)|ICCV|2019|Filter|Search base Pruning,supernet|(https://1m0n-earth.tistory.com/14)|
|2|[Filter pruning via geometric median for deep convoultion neural networks acceleration](https://arxiv.org/abs/1811.00250)|CVPR|2019|Filter||(https://1m0n-earth.tistory.com/12)|
|3|[Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks](https://arxiv.org/abs/1909.08174)|NeurIPS|2019|Filter||(https://1m0n-earth.tistory.com/12)|
|18|[The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)|ICLR|2019|Weight||(https://1m0n-earth.tistory.com/17)|
|4|[HRank: Filter Pruning using High-Rank Feature Map](https://arxiv.org/abs/2002.10179)|CVPR|2020|Filter||(https://1m0n-earth.tistory.com/12)|
|8|[DMCP: Differentiable Markov Channel Pruning for Neural Networks](https://arxiv.org/abs/2005.03354)|CVPR|2020|Filter|diffentiable search|
|14|[Towards Efficient Model Compression via Learned Global Ranking](https://arxiv.org/abs/1904.12368)|CVPR(Oral)|2020|Filter||(https://1m0n-earth.tistory.com/16)|
|16|[Movement Pruning: Adaptive Sparsity by Fine-Tuning](https://arxiv.org/abs/2005.07683)|NeurIPS|2020|Weight|
|9|[Once-for-All: Train One Network and Specialize it for Efficient Deployment](https://arxiv.org/abs/1908.09791)|ICLR|2020|Multi||(https://1m0n-earth.tistory.com/18)|
|7|[BCNet: Searching for Network Width with Bilaterally Coupled Network](https://arxiv.org/abs/2105.10533)|CVPR|2021|Filter|supernet|(https://1m0n-earth.tistory.com/14)|
|21|[Width & Depth Pruning for Vision Transformers](https://ojs.aaai.org/index.php/AAAI/article/view/20222)|AAAI|2022|Width&Depth|Transformer||(https://1m0n-earth.tistory.com/19)|
|11|[Depgraph: towards any structure pruning](https://arxiv.org/abs/2301.12900)|CVPR|2023|Filter||(https://1m0n-earth.tistory.com/15)|

## Model

|읽은 순서|Title|Venue|Year|Model Type|Note|
|:---:|------|:---:|:---:|:------:|----|
|15|[MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381)|CVPR|2019|CNN|





